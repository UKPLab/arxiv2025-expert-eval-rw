{
    "eval_data": {
        "contribution_type": {
            "1": "You will be given abstract, introduction and related work sections of an academic paper. Your task is to revise the related work section, so that each paragraph clearly states the contributions of the current paper and/or its position among the literature. Ensure that contribution and/or positioning statements should be aligned with the specific focus of each paragraph. Start your answer immediately without providing any other explanation.",
            "2": "You will be given abstract, introduction and related work sections of an academic paper. Your task is to revise the related work section, so that each paragraph focuses solely on summarizing and comparing prior research, without mentioning the current paper’s contributions or its position in the literature. Then, add a final paragraph at the end of the related work section that presents the current paper’s contributions and its positioning. While writing the final paragraph, address all aspects mentioned in each previous paragraph. Start your answer immediately without providing any other explanation. Start your answer immediately without providing any other explanation.",
            "3": "You will be given a related work section of an academic paper. Your task is to revise the related work section, so that the contributions of the current paper and/or its position among the literature will not be mentioned, focusing solely on summarizing and comparing prior research. Start your answer immediately without providing any other explanation."
        }
    },
    "eval_test": {
        "coherence": {
            "system_prompt_fs": "You will receive some content from a scientific paper, a sentence that is supposed to cite that paper and a specific citation number. Your task is to determine whether the given paper context supports (entails) the sentence for that specific citation number. In cases where more than one paper is referenced in the sentence, as long as context in which given citation number fits the paper content, it should be count as entailment as well. In multiple citation cases, the paper does not have to entail whole sentence. Some examples showing the implementation of the task will be provided. By utilizing the examples, first provide your reasoning, and then your answer. If the paper context entails the citation sentence, answer \"yes\". If not, answer \"no\". Your output should be in JSON format.",
            "system_prompt_zs": "You will receive some content from a scientific paper, a sentence that is supposed to cite that paper and a specific citation number. Your task is to determine whether the given paper context supports (entails) the sentence for that specific citation number. In cases where more than one paper is referenced in the sentence, as long as context in which given citation number fits the paper content, it should be count as entailment as well. In multiple citation cases, the paper does not have to entail whole sentence. First provide your reasoning, and then your answer. If the paper context entails the citation sentence, answer \"yes\". If not, answer \"no\". Your output should be in JSON format.",
            "example": "<START OF EXAMPLE 1>\n\nPAPER CONTEXT: Neural Architecture Search methods are effective but often use complex algorithms to come up with the best architecture. We propose an approach with three basic steps that is conceptually much simpler. First we train N random architectures to generate N (architecture, validation accuracy) pairs and use them to train a regression model that predicts accuracy based on the architecture. Next, we use this regression model to predict the validation accuracies of a large number of random architectures. Finally, we train the top-K predicted architectures and deploy the model with the best validation result. While this approach seems simple, it is more than 20\u00d7 as sample efficient as Regularized Evolution on the NASBench-101 benchmark and can compete on ImageNet with more complex approaches based on weight sharing, such as ProxylessNAS.\nThe original Neural Architecture Search (NAS) methods have resulted in improved accuracy but they came at a high computational cost [27, 20, 19] . Recent advances have reduced this cost significantly [15, 9, 26, 4, 18, 5, 2, 17, 24, 23, 3] , but many of them require nontrivial specialized implementations. For example, weight sharing introduces additional complexity into the search process, and must be carefully tuned to get good results.\nWith an infinite compute budget, a naive approach to architecture search would be to sample tens or hundreds of thousands of random architectures, train and evaluate each one, and then select the architectures with the best validation set accuracies for deployment; this is a straightforward application of the ubiquitous random search heuristic. However, the computational requirements of this approach makes it infeasible in practice. For example, to exhaustively train and evaluate each of the 400,000 architectures in the NASBench [25] search space, it would take roughly 25 years of TPU training time. Only a small number of companies and corporate research labs can afford this much compute, and it is far out of reach for most ML practitioners.\nOne way to alleviate this is to identify a small subset of promising models. If we can do this with a reasonably high recall (most models selected are indeed of high quality) then we can train and validate just this limited set of models to reliably select a good one for deployment. To achieve this, the proposed Neural Predictor uses the following steps to perform an architecture search:\n(1) Build a predictor by training N random architectures to obtain N (architecture, validation accuracy) pairs. Use this data to train a regression model.\n(2) Quality prediction using the regression model over a large set of random architectures. Select the K most promising architectures for final validation.\n(3) Final validation of the top K architectures by training them. Then we select the model with the highest validation accuracy to deploy.\nThe workflow is illustrated in Figure 1 . In this setup, the first step is a traditional regression problem where we first generate a dataset of N samples to train on. The second step can be carried out efficiently because evaluating a model using the predictor is cheap. The third step is nothing more than traditional validation where we only evaluate a well curated set of K models. While the method outlined above might seem straightforward, it is very effective:\n The Neural Predictor strongly outperforms random search on NASBench-101. It is also about 22.83 times more sample-efficient than Regularized Evolution, the best performing method in the NASBench-101 paper.\n The Neural Predictor can easily handle different search spaces. In addition to NASBench-101, we evaluated it on the ProxylessNAS [4] search space and found that the predicted architecture is as accurate as Proxyless-NAS and clearly better than random search.\n The architecture selection process uses two of the most ubiquitous tools from the ML toolbox: random sampling and supervised learning. In contrast, many existing NAS approaches rely on reinforcement learning, weight sharing, or Bayesian optimization.\n The most computationally intensive components of the proposed method (training N models in step 1 and K models in step 3) are highly parallelizable when sufficient computation resources are available.\n\nCITATION SENTENCE: Additionally, simple predictors that regress on features extracted from architectures, such as textual encoding schemes or direct structural descriptions, have been found effective for performance approximation, enabling significant sample efficiency improvements [9][10].\n\nCITED PAPER: 10\n\nREASONING: The context discusses the effectiveness of a Neural Predictor method that uses regression modeling to predict validation accuracy based on architecture features, achieving significant sample efficiency improvements over traditional approaches. The citation sentence aligns with the context in pointing out the effectiveness of simple predictors for performance approximation, which correlates with the regression steps described in the Neural Predictor method. Therefore, the citation sentence is entailed by the context and follows from it. Therefore, the answer should be \"yes\".\n\nANSWER: Yes\n\n<END OF EXAMPLE 1>\n\n\n<START OF EXAMPLE 2>\n\nPAPER CONTEXT: We present a dialogue generation model that directly captures the variability in possible responses to a given input, which reduces the boring output issue of deterministic dialogue models. Experiments show that our model generates more diverse outputs than baseline models, and also generates more consistently acceptable output than sampling from a deterministic encoder-decoder model.\nThe task of open-domain dialogue generation is an area of active development, with neural sequence-to-sequence models dominating the recently published literature (Shang et al., 2015; Vinyals and Le, 2015; Li et al., 2016b,a; Serban et al., 2016) . Most previously published models train to minimise the negative log-likelihood of the training data, and then at generation time either perform beam search to find the output Y which maximises P (Y |input) (Shang et al., 2015; Vinyals and Le, 2015; Serban et al., 2016 ) (ML decoding), or sample from the resulting distribution (Serban et al., 2016) .\nA notorious issue with ML decoding is that this tends to generate short, boring responses to a wide range of inputs, such as \"I don't know\". These responses are common in the training data, and can be replies to a wide range of inputs (Li et al., 2016a; Serban et al., 2016) . In addition, shorter responses typically have higher likelihoods, and so wide beam sizes often result in very short responses (Tu et al., 2017; Belz, 2007) . To resolve this problem, Li et al. (2016a) propose instead using maximum mutual information with a length boost as a decoding objective, and report more interesting generated responses.\nFurther, natural dialogue is not deterministic; for example, the replies to \"What's your name and where do you come from?\" will vary from person to person. Li et al. (2016b) have proposed learning representations of personas to account for interperson variation, but there can be variation even among a single person's responses to certain questions.\nRecently, Serban et al. (2017) have introduced latent variables to the dialogue modelling framework, to model the underlying distribution over possible responses directly. These models have the benefit that, at generation time, we can sample a response from the distribution by first sampling an assignment of the latent variables, and then decoding deterministically. In this way, we introduce stochasticity without resorting to sampling from the decoder, which can lead to incoherent output -see Table 1 for examples.\nIn this paper, we present a latent variable model for one-shot dialogue response, and investigate what kinds of diversity the latent variables capture. Our experiments show that our model has higher lexical as well as sentential diversity than baseline models. We also show that our model generates more acceptable diverse output than sampling from a deterministic decoder. We end by noting that all three methods proposed above to combat the 'maximum likelihood response' are ways of decreasing the probability of the generated output, and report some preliminary results for how response probability interacts with grammaticality and interestingness.\n\nCITATION SENTENCE: Natural Language Inference has emerged as an essential tool for modeling semantic relationships, with applications extending from textual entailment assessments [11,12] to dialogue consistency verification [17].\n\nCITED PAPER: 11\n\nREASONING: The citation sentence discusses Natural Language Inference (NLI) and its applications, which is not directly related or entailed by the presented context on dialogue generation models and their improvements. The context does not specifically mention or address NLI or its use in dialogue consistency verification. As such, the hypothesis cannot be inferred or entailed from the given context. Therefore, the answer should be \"no\".\n\nANSWER:No\n\n<END OF EXAMPLE 2>"
        },
        "contribution": {
            "system_prompts_fs": {
                "contribution_type": "You will be given a related work section draft for an academic paper. Your task is to determine whether this draft (1) states the main paper's contribution or its position among the literature in each paragraph, or (2) provides the contributions and/or position in the final paragraph as a summary, or (3) does not mention any contributions/position at all. In addition, some examples showing the implementation of the task will be provided. By utilizing the examples, first provide your reasoning, and then your answer. Your answer should be either \"1\" for each paragraph, \"2\" for final paragraph, \"3\" for no contribution. Your output should be in JSON format.",
                "direct_eval": "Your task is to check whether the given paragraph, from a related work section draft for an academic paper, explicitly or implicitly mention the main paper's contribution or position among existing literature. In addition, some examples showing the implementation of the task will be provided. By utilizing the examples, first provide your reasoning, and then your answer as either \"yes\" or \"no\". Your output should be in JSON format.",
                "pairwise_eval": "You will be given two paragraphs, context and final, from a related work section draft for an academic paper. Your task is to check whether the final paragraph states the main paper's contributions or its position among the literature while addressing the points from the context paragraph. In other words, you will check whether contributions in the final paragraph include the discussed points in the context paragraph. In addition, some examples showing the implementation of the task will be provided. By utilizing the examples, first provide your reasoning, and then your answer as either \"yes\" or \"no\". Your output should be in JSON format."
            },
            "system_prompts_zs": {
                "contribution_type": "You will be given a related work section draft for an academic paper. Your task is to determine whether this draft (1) states the main paper's contribution or its position among the literature in each paragraph, or (2) provides the contributions and/or position in the final paragraph as a summary, or (3) does not mention any contributions/position at all. First provide your reasoning, and then your answer. Your answer should be either \"1\" for each paragraph, \"2\" for final paragraph, \"3\" for no contribution. Your output should be in JSON format.",
                "direct_eval": "Your task is to check whether the given paragraph, from a related work section draft for an academic paper, explicitly or implicitly mention the main paper's contribution or position among existing literature. First provide your reasoning, and then your answer as either \"yes\" or \"no\". Your output should be in JSON format.",
                "pairwise_eval": "You will be given two paragraphs, context and final, from a related work section draft for an academic paper. Your task is to check whether the final paragraph states the main paper's contributions or its position among the literature while addressing the points from the context paragraph. In other words, you will check whether contributions in the final paragraph include the discussed points in the context paragraph. First provide your reasoning, and then your answer as either \"yes\" or \"no\". Your output should be in JSON format."
            },
            "examples": {
                "contribution_type": "<START OF EXAMPLE 1>\n\nDRAFT: Previous studies on TDMR (Task-Dataset-Metric-Result) extraction primarily rely on data sources such as NLP-progress or PapersWithCode. While these sources are popular, they lack rigorous quality control, including standardized TDM entity representation and complete publication coverage. Some efforts, such as [1] and [2], go further by extracting TDM triples and result values, and normalizing them for leaderboard construction. However, these approaches are constrained by their reliance on a closed domain, requiring extracted triples to match a pre-defined TDM taxonomy. Other studies, such as [3] and [4], only extract TDM triples without result values, thereby forgoing leaderboard construction. Meanwhile, [6] focuses on extracting result values, but only in the context of pre-defined TDM triples, making it similar in limitation to [1]. Consequently, these methods are not adaptable to the dynamic nature of emerging benchmarks, where new tasks, datasets, and metrics are frequently introduced.\nIn a parallel research thread, scientific leaderboards have been treated as a form of scientific knowledge graphs composed of Task, Dataset, and Metric entities. Information extraction in this context has been widely studied, as seen in works such as [5, 7, 8, 9, 10]. These studies typically focus on linking scientific content to structured entities but often assume that the schema or triples are known in advance.\nIn contrast to prior work, our approach addresses several critical limitations. We construct our dataset manually from publications to ensure comprehensive TDMR annotations, rather than relying on incomplete or unstandardized third-party sources. We normalize TDMR tuples for accurate leaderboard construction and propose novel experimental settings that reflect realistic, dynamic scenarios, including cold-start conditions without any pre-defined TDM taxonomy. By simultaneously constructing a scientific knowledge graph and linking publications to it in an open-domain setting, our method is inherently adaptable to the evolving research landscape.\n\nREASONING: The related work draft does not mention the main paper's contributions and its position in each individual paragraph; instead, it waits until the final paragraph to outline the specific contributions of the study. The contributions are clearly summarized and detailed only in the final paragraph. Therefore, the answer should be \"2\".\n\nANSWER: 2\n\n<END OF EXAMPLE 1>\n\n\n<START OF EXAMPLE 2>\n\nDRAFT: In terms of data source, previous studies use either NLP-progress or paperswithcode. These sources, however, lack rigorous quality assurance, such as standardizing TDM entities across different leaderboards and ensuring complete coverage of relevant publications. Similar to our work, [1] and [2] extract TDM triples along with the results values and apply normalization for leaderboard construction. However, both studies assume a closed domain and match extracted TDM triples to a pre-defined TDM triple taxonomy. On the other hand, some studies only partially extract TDMR tuples and do not apply normalization. For example, [3] and [4] extract TDM triples without results. Therefore, these works do not deal with leaderboard construction. In addition, [6] extract the results values depending on the pre-defined TDM triples. Both [3] and [6] leverage pre-defined TDM triples in an extraction process similar to [1]. Since these approaches require a pre-defined taxonomy of TDM triples, they are incompatible with a realistic task definition. In short, none of the previous work is adaptable to the constantly emerging benchmarks driven by new research and innovation. In this work, we address the aforementioned problems. Unlike previous work, we (1) manually construct our dataset directly from publications to ensure complete TDMR annotations, (2) apply normalization for leaderboard construction, and (3) propose different experimental settings to simulate real-world scenarios.\n\nPart of the scientific leaderboards can be viewed as a special type of scientific knowledge graph that includes three types of entities (Task, Dataset, Metric) and the relations between them, which have been the primary focus of the previous studies on information extraction from scientific literature [5, 7, 8, 9, 10]. Our work in the cold start scenario, in which we do not assume any pre-defined TDM triple is given, constructs such a scientific knowledge graph and links the papers to the nodes in the graph simultaneously.\n\nREASONING: Contributions and position of the paper in the literature are mentioned within each paragraph rather than being summarized at the end. The first paragraph discusses previous work and their limitations, then explicitly states what the current work does differently, listing three specific contributions. The second paragraph again discusses previous studies but also explains how the current work differs by constructing a knowledge graph without assuming predefined triples and linking papers to nodes. Therefore, the answer should be \"1\".\n\nANSWER: 1\n\n<END OF EXAMPLE 2>\n\n\n<START OF EXAMPLE 3>\n\nDRAFT: Several efforts have been made to automate the extraction and organization of performance results from scientific publications. Early work by [1] introduced a framework, TDMS-IE, for identifying task, dataset, metric, and score tuples from NLP papers, establishing a foundation for automatic leaderboard construction. Similarly, AxCell [2] proposes a robust pipeline that utilizes structural analysis and novel table segmentation techniques to extract results from machine learning papers, demonstrating significant improvements over prior methods. ORKG-Leaderboards [3] offers a systematic approach that integrates leaderboard extraction into a knowledge graph framework, enabling machine-actionable publishing and dynamic visualization of state-of-the-art performance. TELIN [4] focuses on extracting leaderboard-relevant entities from PDFs using a semi-automated approach that reduces human annotation needs through targeted entity refinement.\n\nIn addition to these extraction systems, several datasets and benchmarks have been introduced to facilitate the development and evaluation of leaderboard construction tools. LEGOBench [5] provides a large-scale benchmark derived from arXiv and PapersWithCode, and evaluates both language model-based and graph-based approaches. SciERC and its associated framework SciIE [6] support multi-task extraction of entities and relations, enabling construction of scientific knowledge graphs. SciREX [7] extends information extraction to the document level, capturing relationships that span across sections, which is critical for leaderboard generation from full papers.\n\nOther contributions include TDMSci [8], a specialized corpus annotated with task, dataset, and metric entities, which supports the development of more accurate extraction models. SciNLP-KG [9] introduces methods for extracting entity relations from NLP literature to build a knowledge graph, highlighting its potential use in leaderboard automation. Additionally, a diachronic analysis of NLP research trends by [10] shows the evolving influence of tasks, methods, and datasets, providing a broader context for understanding the dynamic nature of scientific benchmarks.\n\nREASONING: The related work section draft provided does not explicitly mention or discuss the main paper's contributions or its position. Each paragraph focuses on summarizing existing research efforts and methodologies without indicating how the current paper builds upon or differs from these works. Additionally, the final paragraph does not serve as a summary of the main paper's contributions; instead, it continues to discuss other related works without tying them back to the current study's advancements. Therefore, the answer should be \"3\".\n\nANSWER: 3\n\n<END OF EXAMPLE 3>",
                "direct_eval": "<START OF EXAMPLE 1>\n\nDRAFT: In terms of data source, previous studies use either NLP-progress or paperswithcode. These sources, however, lack rigorous quality assurance, such as standardizing TDM entities across different leaderboards and ensuring complete coverage of relevant publications. Similar to our work, [1] and [2] extract TDM triples along with the results values and apply normalization for leaderboard construction. However, both studies assume a closed domain and match extracted TDM triples to a pre-defined TDM triple taxonomy. On the other hand, some studies only partially extract TDMR tuples and do not apply normalization. For example, [3] and [4] extract TDM triples without results. Therefore, these works do not deal with leaderboard construction. In addition, [6] extract the results values depending on the pre-defined TDM triples. Both [3] and [6] leverage pre-defined TDM triples in an extraction process similar to [1]. Since these approaches require a pre-defined taxonomy of TDM triples, they are incompatible with a realistic task definition. In short, none of the previous work is adaptable to the constantly emerging benchmarks driven by new research and innovation. In this work, we address the aforementioned problems. Unlike previous work, we (1) manually construct our dataset directly from publications to ensure complete TDMR annotations, (2) apply normalization for leaderboard construction, and (3) propose different experimental settings to simulate real-world scenarios.  \n\nPart of the scientific leaderboards can be viewed as a special type of scientific knowledge graph that includes three types of entities (Task, Dataset, Metric) and the relations between them, which have been the primary focus of the previous studies on information extraction from scientific literature [5, 7, 8, 9, 10]. Our work in the cold start scenario, in which we do not assume any pre-defined TDM triple is given, constructs such a scientific knowledge graph and links the papers to the nodes in the graph simultaneously.\n\nREASONING: The draft states the main paper's contribution and how it differs from existing literature. It outlines the limitations of previous studies and then explicitly states how the current work addresses these issues through specific contributions, such as manual dataset construction and handling cold start scenarios without pre-defined TDM triples. Therefore, the answer should be \"yes\".\n\nANSWER: Yes\n\n<END OF EXAMPLE 1>\n\n\n<START OF EXAMPLE 2>\n\nDRAFT: Several efforts have been made to automate the extraction and organization of performance results from scientific publications. Early work by [1] introduced a framework, TDMS-IE, for identifying task, dataset, metric, and score tuples from NLP papers, establishing a foundation for automatic leaderboard construction. Similarly, AxCell [2] proposes a robust pipeline that utilizes structural analysis and novel table segmentation techniques to extract results from machine learning papers, demonstrating significant improvements over prior methods. ORKG-Leaderboards [3] offers a systematic approach that integrates leaderboard extraction into a knowledge graph framework, enabling machine-actionable publishing and dynamic visualization of state-of-the-art performance. TELIN [4] focuses on extracting leaderboard-relevant entities from PDFs using a semi-automated approach that reduces human annotation needs through targeted entity refinement.\n\nIn addition to these extraction systems, several datasets and benchmarks have been introduced to facilitate the development and evaluation of leaderboard construction tools. LEGOBench [5] provides a large-scale benchmark derived from arXiv and PapersWithCode, and evaluates both language model-based and graph-based approaches. SciERC and its associated framework SciIE [6] support multi-task extraction of entities and relations, enabling construction of scientific knowledge graphs. SciREX [7] extends information extraction to the document level, capturing relationships that span across sections, which is critical for leaderboard generation from full papers.\n\nOther contributions include TDMSci [8], a specialized corpus annotated with task, dataset, and metric entities, which supports the development of more accurate extraction models. SciNLP-KG [9] introduces methods for extracting entity relations from NLP literature to build a knowledge graph, highlighting its potential use in leaderboard automation. Additionally, a diachronic analysis of NLP research trends by [10] shows the evolving influence of tasks, methods, and datasets, providing a broader context for understanding the dynamic nature of scientific benchmarks.\n\nREASONING: The draft provides a comprehensive overview of related work but does not explicitly or implicitly mention the main paper's own contribution or how it differs from the existing literature. It talks about the efforts made but doesn't bridge into how this work builds upon them or addresses gaps they left. It focuses solely on summarizing prior work without establishing the unique position or advancement made by the current study. Therefore, the answer should be \"no\". \n\nANSWER: No\n\n<END OF EXAMPLE 2>",
                "pairwise_eval": "<START OF EXAMPLE 1>\n\nCONTEXT: In a parallel research thread, scientific leaderboards have been treated as a form of scientific knowledge graphs composed of Task, Dataset, and Metric entities. Information extraction in this context has been widely studied, as seen in works such as [5, 7, 8, 9, 10]. These studies typically focus on linking scientific content to structured entities but often assume that the schema or triples are known in advance.\n\nFINAL: In contrast to prior work, our approach addresses several critical limitations. We construct our dataset manually from publications to ensure comprehensive TDMR annotations, rather than relying on incomplete or unstandardized third-party sources. We normalize TDMR tuples for accurate leaderboard construction and propose novel experimental settings that reflect realistic, dynamic scenarios, including cold-start conditions without any pre-defined TDM taxonomy. By simultaneously constructing a scientific knowledge graph and linking publications to it in an open-domain setting, our method is inherently adaptable to the evolving research landscape.\n\n\nREASONING: The context paragraph discusses prior work on scientific leaderboards as knowledge graphs with entities like Task, Dataset, and Metric (TDM). It highlights that prior studies focus on information extraction and linking to structured entities but usually assume a pre-existing schema or set of triples. The final paragraph contrasts the main paper's contributions with this prior work by constructing a scientific knowledge graph and linking publications to it in an open-domain and adaptable manner, which directly relates to the context's theme of schema-free extraction and linking. The final paragraph clearly positions the paper among existing literature and addresses the specific limitation mentioned in the context—i.e., the assumption of known schema—by proposing methods that work without one. Therefore, the answer should be \"yes\".\n\nANSWER: Yes\n\n<END OF EXAMPLE 1>\n\n\n<START OF EXAMPLE 2>\n\nCONTEXT: In a parallel research thread, scientific leaderboards have been treated as a form of scientific knowledge graphs composed of Task, Dataset, and Metric entities. Information extraction in this context has been widely studied, as seen in works such as [5, 7, 8, 9, 10]. These studies typically focus on linking scientific content to structured entities but often assume that the schema or triples are known in advance.\n\nFINAL: Building on existing lines of research, this work explores TDMR extraction and scientific leaderboard construction. The dataset is compiled from scientific publications. The approach is designed to accommodate variations in how benchmark results are reported.\n\nREASONING: The context paragraph discusses prior research treating scientific leaderboards as knowledge graphs with a focus on information extraction where the schema is often assumed to be known. The main point raised is that existing methods typically presume a known structure for the extracted information. While the final paragraph mentions the extraction of TDMR and the construction of leaderboards—aligning with the topic in the context—it does not explicitly address whether this work assumes a known schema or introduces any method for schema discovery or flexibility. Thus, the final paragraph does not clearly state how it handles the assumption of a known schema, which is a key point of the context paragraph. Therefore, the answer should be \"no\". \n\nANSWER: No\n\n<END OF EXAMPLE 2>"
            }
        }
    }
}