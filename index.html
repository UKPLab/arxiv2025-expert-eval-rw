<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Expert Preference-based Evaluation of Automated Related Work Generation</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Expert Preference-based Evaluation of Automated Related Work Generation</h1>
            <img src="static/images/tuda_ukp_logo.png" style="width: 50%";>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://furkansahinuc.github.io/" target="_blank">Furkan Şahinuç</a>,</span>
                <span class="author-block">
                  <a href="https://subha0009.github.io/" target="_blank">Subhabrata Dutta</a>,</span>
                  <span class="author-block">
                    <a href="https://www.informatik.tu-darmstadt.de/ukp/ukp_home/head_ukp/index.en.jsp" target="_blank">Iryna Gurevych</a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
<!--                    <span class="author-block">Institution Name<br>Conference name and year</span>-->
                        <span class="author-block">Ubiquitous Knowledge Processing Lab (UKP Lab)<br>Technical University of Darmstadt<br><a href="www.ukp.tu-darmstadt.de">www.ukp.tu-darmstadt.de</a></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv link -->
                      <span class="link-block">
                            <a href="https://arxiv.org/abs/2508.07955" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="ai ai-arxiv"></i>
                        </span>
                        <span>arXiv</span>
                      </a>
                    </span>

                    <!-- Dataset link -->
                    <span class="link-block">
                      <a href="https://tudatalib.ulb.tu-darmstadt.de/handle/tudatalib/4700" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-database"></i>
                      </span>
                      <span>Dataset</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/UKPLab/arxiv2025-expert-eval-rw" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
<!--                &lt;!&ndash; Paper link &ndash;&gt;-->
<!--                <span class="link-block">-->
<!--                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"-->
<!--                  class="external-link button is-normal is-rounded is-dark">-->
<!--                  <span class="icon">-->
<!--                    <i class="ai ai-arxiv"></i>-->
<!--                  </span>-->
<!--                  <span>arXiv</span>-->
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Teaser image-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/Teaser.svg"/>
      <h2 class="subtitle has-text-centered">
        We propose a multi-turn evaluation framework that integrates classical related work evaluation criteria with expert specific preferences.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser image -->


<!--&lt;!&ndash; Teaser video&ndash;&gt;-->
<!--<section class="hero teaser">-->
<!--  <div class="container is-max-desktop">-->
<!--    <div class="hero-body">-->
<!--      <video poster="" id="tree" autoplay controls muted loop height="100%">-->
<!--        &lt;!&ndash; Your video here &ndash;&gt;-->
<!--        <source src="static/videos/banner_video.mp4"-->
<!--        type="video/mp4">-->
<!--      </video>-->
<!--      <h2 class="subtitle has-text-centered">-->
<!--        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus. -->
<!--      </h2>-->
<!--    </div>-->
<!--  </div>-->
<!--</section>-->
<!--&lt;!&ndash; End teaser video &ndash;&gt;-->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Expert domain writing, such as scientific writing, typically demands extensive domain knowledge. Recent advances
              in large language models (LLMs) show promising potential in automating this process, reducing the expert workload.
              However, evaluating the quality of automatically generated scientific writing is a crucial open issue, as it
              requires knowledge of domain-specific evaluation criteria and the ability to discern expert preferences. Conventional
              task-agnostic automatic evaluation metrics and LLM-as-a-judge systems—primarily designed for mainstream NLP
              tasks—are insufficient to grasp expert preferences and domain-specific quality standards. To address this gap
              and support realistic human-AI collaborative writing, we focus on related work generation, one of the most
              challenging scientific tasks, as an exemplar. We propose GREP, a multi-turn evaluation framework that integrates
              classical related work evaluation criteria with expert-specific preferences. Instead of assigning a single
              overall score, our framework decomposes the evaluation into smaller fine-grained dimensions. This localized
              evaluation approach is further augmented with contrastive few-shot examples to provide detailed contextual
              guidance for the evaluation dimensions. The design principles allow our framework to deliver cardinal assessment
              of quality, which can theoretically facilitate better post-training compared to ordinal preference data.
              For better accessibility, we design two variants of GREP: a more precise variant with proprietary LLMs as
              evaluators, and a cheaper alternative with open-weight LLMs.  Empirical investigation reveals that our framework
              is able to assess the quality of related work sections in a much more robust manner compared to standard LLM
              judges, reflects natural scenarios of scientific writing, and bears a strong correlation with the assessment
              of human experts. We also observe that generations from state-of-the-art LLMs struggle to satisfy validation
              constraints of a suitable related work section. They (mostly) fail to improve based on feedback as well.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<section class="section">
    <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <h2 class="title is-3">Project Summary</h2>
                <div class="content has-text-justified">
                    <p> The project consists of two stages. We first select the evaluator models via preliminary evaluation
                        experiments with contrastive few-shot examples. Then, we run our pipeline which employs an iterative
                        algorithm where generation and evaluation are interleaved, simulating multi-turn human-AI interaction.</p>
                </div>
            </div>
        </div>
    </div>
</section>


<section class="section hero is-light">
    <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <h2 class="title is-4">Localized Evaluation Based on Expert Preferences</h2>
                <div class="content has-text-justified">
                    <p style="font-size: 20px;"> We design <span style="color: red;"><em><b>GREP</b></em></span>
                        (<span style="color: red;"><b>G</b></span>ranular <span style="color: red;"><b>R</b></span>elated-work
                        <span style="color: red;"><b>E</b></span>valuation based on <span style="color: red;"><b>P</b></span>references)
                        a fine-grained, multi-turn evaluation system to assess the quality of generated RW sections and the ability of
                        the generator to cater to evaluation feedback (figure on the right). Our evaluation rubric consists of hard
                        constraints (i.e., necessary to fulfill  to be considered as a valid RW section, e.g., no omitted  paper, no
                        hallucination, coherent citation, etc.) as well as soft constraints (i.e., reflect human preferences when multiple
                        valid RW sections are possible, e.g., internal structuring, emphasis on certain cited papers, etc.)</p>
                </div>
            </div>
            <img src="static/images/Framework.webp"/>
        </div>
    </div>
</section>


<section class="section">
    <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <h2 class="title is-4">Contrastive Few-Shot Examples</h2>
                <div class="content has-text-justified">
                    <p style="font-size: 20px;">Our preliminary experiments show that applying vanilla zero-shot LLM-as-a-judge
                        remains insufficient for such expert domain evaluations. We identify the main reason as the absence of
                        context information indicating a specific evaluation criteria and what does it mean to satisfy (or not)
                        that (It is trivial that few-shot examples improve classification. However, such examples cannot be
                        presented if one uses an end-to-end judge, due to context-length bottleneck). For each possible outcome
                        of a specific evaluation, we include an example along with a reasoning component that explains the expected
                        outcome. Since finding failing examples for specific aspects is non-trivial, we generate synthetic examples
                        using LLMs prompted to make deliberate mistakes.</p>
                </div>
            </div>
            <div class="column is-four-fifths">
            <img src="static/images/Llm-as-judge.svg"/>
            </div>
        </div>
    </div>
</section>


<section class="section hero is-light">
    <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <h2 class="title is-4">Paper Information</h2>
                <div id="imported-content"></div>
                <script>
                    fetch('static/example_paper_info.html')
                      .then(response => response.text())
                      .then(html => {
                        document.getElementById('imported-content').innerHTML = html;
                      })
                </script>
            </div>
            <div class="column is-four-fifths">
            <h2 class="title is-4">Example Generated Draft</h2>
                <div class="content has-text-justified">
                    <p>Recent work on bilingual training has demonstrated the benefits of inducing aligned word embeddings that mitigate data sparsity. For example, [1] employs a multitask framework leveraging co-occurrence statistics from parallel data to generate shared representations, while [2] extends the distributional hypothesis to multilingual settings by inducing joint-space embeddings that capture compositional semantics without explicit word alignments. These approaches establish a solid foundation for our model, which harnesses bilingual training to capture both semantic representations and context-dependent language production in a grounded color reference task.</p>
                    <p>Additional research has illustrated the effectiveness of sharing representations across languages with minimal architectural changes. In [3], the use of an artificial token to indicate the target language enabled a single neural machine translation model to perform zero-shot translation, illustrating implicit crosslingual transfer. Moreover, [4] demonstrates that a unified multi-task model spanning diverse domains attains competitive performance by learning shared representations. These insights motivate our bilingual strategy, where a shared vocabulary supports the generation of language-specific utterances while benefiting from cross-lingual inductive biases.</p>
                    <p>In summary, our work draws on the complementary strengths of bilingual representation learning from [1] and [2] and crosslingual sharing techniques from [3] and [4]. We introduce a bilingual model for color reference games that not only exhibits human-like contextual sensitivity and improves pragmatic informativeness but also faithfully captures language-specific semantic distinctions. This contribution extends the current literature on bilingual and multilingual models in grounded communication tasks by effectively integrating semantic understanding with pragmatic language production.</p>
                </div>
            </div>

            <div class="column is-four-fifths">
                <h2 class="title is-4">Evaluation Report</h2>
                <div class="content has-text-justified">
                    <!-- MISSING CITATIONS -->
                    <section class="mb-4">
                        <p><strong>Missing Citations:</strong> None</p>
                    </section>
                    <!-- HALLUCINATED CITATIONS -->
                    <section class="mb-4">
                        <p><strong>Hallucinated Citations:</strong> None</p>
                    </section>

                    <!-- LENGTH -->
                    <section class="mb-4">
                        <p><strong>Length:</strong> Adequate</p>
                    </section>

                    <!-- CITATION EMPHASIS EVALUATION -->
                    <section class="mb-4">
                        <h3 class="subtitle is-5">Citation Emphasis</h3>
                        <ul>
                        <li><strong>Cited Paper 1:</strong> Excessive emphasis</li>
                        <li><strong>Cited Paper 2:</strong> Excessive emphasis</li>
                        <li><strong>Cited Paper 3:</strong> Excessive emphasis</li>
                        <li><strong>Cited Paper 4:</strong> Excessive emphasis</li>
                        </ul>
                    </section>

                    <!-- COHERENCE PROBLEMATIC SENTENCES -->
                    <section class="mb-4">
                    <h3 class="subtitle is-5">Coherence</h3>
                    <div class="content">
                      <p>Cited Paper: 4</p>
                      <p>Sentence: In summary, our work draws on the complementary strengths of bilingual representation
                          learning from [1] and [2] and crosslingual sharing techniques from [3] and [4].</p>
                      <p>Reasoning: The given paper context discusses advancements in multi-task and multi-domain deep
                          learning models, including the MultiModel architecture and its application to translation tasks,
                          but does not explicitly address cross-lingual sharing techniques or bilingual representation
                          learning. Consequently, the citation sentence referring to cross-lingual sharing techniques is
                          not supported or entailed by the paper's content.</p>
                    </div>
                    </section>

                    <!-- CONTRIBUTION - POSITIONING EVALUATION -->
                    <section>
                    <h3 class="subtitle is-5">Positioning</h3>
                    <div class="content">
                        <p><strong>Positioning Type Result:</strong> Matched</p>
                      <p><strong>Positioning Type Reasoning:</strong> The paper lays its foundation by reviewing prior work and proposing its approach within that context, but it only explicitly summarizes its contributions—specifically, the introduction of a bilingual model for color reference games—in the final paragraph.</p>
                    </div>
                        <p><strong>Positioning Problematic Paragraphs:</strong> None</p>
                    </section>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
        <h2 class="title is-3">Expert Evaluation</h2>
        <p> We implement an expert evaluation study to validate the automated assessment. Human experts interact with a
            pair of generator models simultaneously, for three iterations. At each iteration, the experts evaluate the
            generated drafts in terms of coherence, positioning, and feedback (instruction) following capabilities, and
            provide feedback to the models independently. </p>
        <div class="item item-video1">
            <br>
            <video poster="" id="video1" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="static/videos/human_eval.webm"
            type="video/mp4">
            </video>
        </div>
    </div>
  </div>
</section>


<section class="section hero is-light">
    <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <h2 class="title is-4">Results - Takeaways</h2>
                <div class="content has-text-justified">
                    <p style="font-size: 20px;">Experiments unravel fundamental limitations of SoTA LLMs as RW section
                        generator: they struggle to coherently cite prior work (the best performing model, improvement
                        upon explicit feedback is rare, and they struggle to incorporate even simple preference-based
                        instructions like adjusting the length of the generated RW section.<br>
                        While specialized SoTA LLM judge delivers near-random matching with expert judgments (e.g., 53%
                        match in terms of citation coherence), automated assessments from PreciseGREP and OpenGREP provide
                        judgments that are closely similar to experts, e.g., matching 78% and 66% in terms of evaluating
                        citation coherence, respectively.</p>
                </div>
            </div>
            <div class="column is-four-fifths">
                <img src="static/images/Full_Pipeline_Result.svg"/>
                <img src="static/images/Human_Eval_Result.png"/>
            </div>
        </div>
    </div>
</section>

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{sahinuc2025expertEval,
    title       = {Expert Preference-based Evaluation of Automated Related Work Generation},
    author      = {Furkan \c{S}ahinu\c{c} and Subhabrata Dutta and Iryna Gurevych},
    year        = {2025},
    eprint      = {2508.07955},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url         = {https://arxiv.org/abs/2508.07955},
}
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
          </p>
        </div>
      </div>
    </div>
  </div>
  </footer>

</body>
</html>
